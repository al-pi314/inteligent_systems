{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from classifiers.random_classifier import RandomClassifier\n",
    "from classifiers.majority_classifier import MajorityClassifier\n",
    "from classifiers.naive_bayes_classifier import NaiveBayesClassifier\n",
    "from classifiers.logistic_regression_classifier import LogisticRegressionClassifier\n",
    "from classifiers.decision_tree_classifier import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./seminar_2/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./seminar_2/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = train_df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class 1: \", class_counts[1], \"percentage: \", class_counts[1]/len(train_df))\n",
    "print(\"Class 2: \", class_counts[2], \"percantage:\", class_counts[2]/len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.pie(x=class_counts, labels=class_counts.index, autopct='%1.1f%%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is binary, with the value 1 indicating that the chemical is bio-degradable and 2 indicating that it is not bio-degradable. The dataset is imbalanced, with 1's representing 66.7% of the data and 2's representing 33.3% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = train_df.isnull().sum(axis = 0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.suptitle('NaNs in columns', fontsize=16)\n",
    "plt.bar(nans.index, nans.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few NaN values in the dataset, but not a lot. We assume that dropping these rows will not have a significant impact on the model, but we will also test the model with imputation such as taking the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_in_data = train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_to_class = correlation_in_data[\"Class\"]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.suptitle('Correlation to class vairable', fontsize=16)\n",
    "plt.bar(correlation_to_class.index, correlation_to_class.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No features have a very high direct correlation to the target variable, but quite a lot of features have some correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_in_data, fmt=\".2f\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most features are not directly correlated to one another, but there are some brighter spots on the heatmap indicating some correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_columns = set()\n",
    "threshold = 0.75\n",
    "\n",
    "for i in range(len(correlation_in_data.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_in_data.iloc[i, j]) > threshold:\n",
    "            colname1 = correlation_in_data.columns[i]\n",
    "            colname2 = correlation_in_data.columns[j]\n",
    "            correlated_columns.add((colname1, colname2, correlation_in_data.iloc[i, j]))\n",
    "\n",
    "print(correlated_columns)\n",
    "print(len(correlated_columns), \"highly correlated features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_without_index_and_class = train_df.drop([\"Index\", \"Class\"], axis=1)\n",
    "plt.bar(train_df_without_index_and_class.nunique().index, train_df_without_index_and_class.nunique().values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Number of unique values in columns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_without_index_and_class.boxplot(figsize=(10, 10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Outliers of all columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the distribution of the features, we can see that most features have some outlies. We will test the model with and without outlier removal, we assume that removing the outliers will have a significant impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns =  [i for i in train_df_without_index_and_class.nunique().index.where(train_df_without_index_and_class.nunique().values >= 100) if i is not None]\n",
    "\" \".join(continuous_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that we will test models with differently preprocessed data to see which preprocessing method works best. We will test the following preprocessing methods:\n",
    "- Dropping NaN values\n",
    "- Replacing NaN values with the mean value\n",
    "- Dropping outliers\n",
    "- Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.drop([\"Index\"], axis=1)\n",
    "without_nan = train_data.dropna(axis=0)\n",
    "nan_replaced = train_data.fillna(without_nan.mean())\n",
    "without_outliers = without_nan[(np.abs(stats.zscore(without_nan)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = PolynomialFeatures(degree=3)\n",
    "poli_data = trans.fit_transform(without_nan.drop([\"Class\"], axis=1))\n",
    "size = poli_data.shape[1]\n",
    "poli_data = np.hstack((poli_data, without_nan[\"Class\"].values.reshape(-1, 1)))\n",
    "poli_data = pd.DataFrame(poli_data, columns=[f\"poly_{i}\" for i in range(size)] + [\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_df.drop([\"Index\"], axis=1)\n",
    "test_data_without_nan = test_data.dropna(axis=0)\n",
    "test_data_nan_replaced = test_data.fillna(test_data_without_nan.mean())\n",
    "test_data_without_outliers = test_data_without_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_test = trans.transform(test_data_without_nan.drop([\"Class\"], axis=1))\n",
    "size = poli_test.shape[1]\n",
    "poli_test = np.hstack((poli_test, test_data_without_nan[\"Class\"].values.reshape(-1, 1)))\n",
    "poli_test = pd.DataFrame(poli_test, columns=[f\"poly_{i}\" for i in range(size)] + [\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, target_column):\n",
    "    return data.drop([target_column], axis=1), data[target_column]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to test the following models:\n",
    "- Random classifier (as a baseline)\n",
    "- Majority classifier (as a baseline)\n",
    "- Naive Bayes classifier (because it is fast and simple)\n",
    "- Logistic regression (because it is good for binary classification)\n",
    "- Decision tree (because it is good for high dimensional data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_target = split_data(train_data, \"Class\")\n",
    "best_rnd = RandomClassifier(train_features, train_target)\n",
    "best_rnd_data = train_data.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_target = split_data(train_data, \"Class\")\n",
    "maj_classifier = MajorityClassifier(train_features, train_target)\n",
    "best_maj_data = train_data.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnan_features, wnan_target = split_data(without_nan, \"Class\")\n",
    "wnan_test_features, wnan_test_target = split_data(test_data_without_nan, \"Class\")\n",
    "without_nan_nb = NaiveBayesClassifier(wnan_features, wnan_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features, mean_target = split_data(nan_replaced, \"Class\")\n",
    "mean_test_features, mean_test_target = split_data(test_data_nan_replaced, \"Class\")\n",
    "mean_nb = NaiveBayesClassifier(mean_features, mean_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slight_smoothing_nan_nb = NaiveBayesClassifier(wnan_features, wnan_target, var_smoothing=10e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heavy_smoothing_nan_nb = NaiveBayesClassifier(wnan_features, wnan_target, var_smoothing=10e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_features, outliers_target = split_data(without_outliers, \"Class\")\n",
    "outliers_test_features, outliers_test_target = split_data(test_data_without_outliers, \"Class\")\n",
    "outliers_nb = NaiveBayesClassifier(outliers_features, outliers_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_features, poli_target = split_data(poli_data, \"Class\")\n",
    "poli_test_features, poli_test_target = split_data(poli_test, \"Class\")\n",
    "poli_nb = NaiveBayesClassifier(poli_features, poli_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifiers = [(without_nan_nb, test_data_without_nan), (mean_nb, test_data_nan_replaced), (slight_smoothing_nan_nb, test_data_without_nan), (heavy_smoothing_nan_nb, test_data_without_nan), (outliers_nb, test_data_without_outliers), (poli_nb, poli_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.empty((len(nb_classifiers), 5))\n",
    "for i, (c, test_data) in enumerate(nb_classifiers):\n",
    "    f, t = split_data(test_data, \"Class\")\n",
    "    scores[i] = c.evaluate(f, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "score_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]\n",
    "classifiers = [\"Wo NaN\", \"Mean\", \"Slight S\", \"Heavy S\", \"Outliers\", \"Poly\"]\n",
    "for i in range(5):\n",
    "    axes[i].bar([i for i in range(len(nb_classifiers))], scores[:, i])\n",
    "    axes[i].set_title(score_names[i])\n",
    "    axes[i].set_xticks([i for i in range(len(nb_classifiers))])\n",
    "    axes[i].set_xticklabels(classifiers)\n",
    "    axes[i].xaxis.set_tick_params(rotation=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnan_features, wnan_target = split_data(without_nan, \"Class\")\n",
    "without_nan_lr = LogisticRegressionClassifier(wnan_features, wnan_target, solver='lbfgs', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features, mean_target = split_data(nan_replaced, \"Class\")\n",
    "mean_lr = LogisticRegressionClassifier(mean_features, mean_target, solver='lbfgs', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_penalty_lr = LogisticRegressionClassifier(mean_features, mean_target, solver='liblinear', max_iter=1000, penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_lr = LogisticRegressionClassifier(mean_features, mean_target, solver='lbfgs', max_iter=1000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_features, outliers_target = split_data(without_outliers, \"Class\")\n",
    "outliers_lr = LogisticRegressionClassifier(outliers_features, outliers_target, solver='lbfgs', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_features, poli_target = split_data(poli_data, \"Class\")\n",
    "poli_lr = LogisticRegressionClassifier(poli_features, poli_target, solver='lbfgs', max_iter=1000, tol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifiers = [(without_nan_lr, test_data_without_nan), (mean_lr, test_data_nan_replaced), (L1_penalty_lr, test_data_nan_replaced), (balanced_lr, test_data_nan_replaced), (outliers_lr, test_data_without_outliers), (poli_lr, poli_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "score_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]\n",
    "classifiers = [\"Wo NaN\", \"Mean\", \"L1\", \"Balanced\", \"Outliers\", \"Poly\"]\n",
    "for i in range(5):\n",
    "    axes[i].bar([i for i in range(len(lr_classifiers))], scores[:, i])\n",
    "    axes[i].set_title(score_names[i])\n",
    "    axes[i].set_xticks([i for i in range(len(lr_classifiers))])\n",
    "    axes[i].set_xticklabels(classifiers)\n",
    "    axes[i].xaxis.set_tick_params(rotation=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnan_features, wnan_target = split_data(without_nan, \"Class\")\n",
    "without_nan_dt = DecisionTreeClassifier(wnan_features, wnan_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features, mean_target = split_data(nan_replaced, \"Class\")\n",
    "mean_dt = DecisionTreeClassifier(mean_features, mean_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_df = DecisionTreeClassifier(mean_features, mean_target, random_state=42, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df = DecisionTreeClassifier(mean_features, mean_target, random_state=42, criterion='entropy', ccp_alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_features, outliers_target = split_data(without_outliers, \"Class\")\n",
    "outliers_dt = DecisionTreeClassifier(outliers_features, outliers_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_features, poli_target = split_data(poli_data, \"Class\")\n",
    "poli_dt = DecisionTreeClassifier(poli_features, poli_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = [(without_nan_dt, test_data_without_nan), (mean_dt, test_data_nan_replaced), (limited_df, test_data_nan_replaced), (cc_df, test_data_nan_replaced), (outliers_dt, test_data_without_outliers), (poli_dt, poli_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "score_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]\n",
    "classifiers = [\"Wo NaN\", \"Mean\", \"Limited\", \"CC\", \"Outliers\", \"Poly\"]\n",
    "for i in range(5):\n",
    "    axes[i].bar([i for i in range(len(dt_classifier))], scores[:, i])\n",
    "    axes[i].set_title(score_names[i])\n",
    "    axes[i].set_xticks([i for i in range(len(dt_classifier))])\n",
    "    axes[i].set_xticklabels(classifiers)\n",
    "    axes[i].xaxis.set_tick_params(rotation=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = [(best_rnd, best_rnd_data), (maj_classifier, best_maj_data), (outliers_nb, test_data_without_outliers), (outliers_lr, test_data_without_outliers), (outliers_dt, test_data_without_outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from testing of the models above removing the outliers had the most significant impact on the model. We will further test these models using folding and multiple runs to see if the results are consistent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = 10\n",
    "folds = 5\n",
    "evaluations = 5\n",
    "scores = np.empty(shape=(len(best_models), repetitions, evaluations))\n",
    "for i, (classifier, model_data) in enumerate(best_models):\n",
    "    scores[i] = classifier.test(model_data, \"Class\", folds=folds, repetitions=repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_names = [\"F1 score\", \"Precision\", \"Recall\", \"Area under ROC curve\", \"Accuracy\"]\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i in range(len(score_names)):\n",
    "    r = (i + 1) // 3\n",
    "    c = (i + 1) % 3\n",
    "    ax[r, c].set_title(score_names[i])\n",
    "    ax[r, c].plot(scores[0, :,  i], label=\"Random\")\n",
    "    ax[r, c].plot(scores[1, :,  i], label=\"Majority\")\n",
    "    ax[r, c].plot(scores[2, :,  i], label=\"Naive Bayes\")\n",
    "    ax[r, c].plot(scores[3, :,  i], label=\"Logistic Regression\")\n",
    "    ax[r, c].plot(scores[4, :,  i], label=\"Decision Tree\")\n",
    "    if i == 0:\n",
    "        ax[r, c].legend(loc=(-1.2, 0.2), prop={'size': 15})\n",
    "\n",
    "    ax[r, c].set_ylim(0.4, 1.05)\n",
    "ax[0, 0].axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected models outpreformed the baseline models. In the recall metric majority classifier was best due to how the metric is calculated (majority classifier cannot produce false negatives as the majority class in training data is positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(score_names), figsize=(len(score_names) * 4, 5))\n",
    "\n",
    "for i in range(len(score_names)):\n",
    "    ax[i].set_title(score_names[i])\n",
    "    ax[i].bar([\"RND\", \"MAJ\", \"NB\", \"LR\", \"DT\"], scores.mean(axis=1)[:, i], color=sns.color_palette(\"Set2\", 10))\n",
    "    ax[i].set_ylim(0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose logistic regression because it preforms best in most metrics (but not by a large margin), but we also decided to test Naive Bayes because for our problem Precision is very important, we do not want to classify non-biodegradable chemicals as biodegradable. Naive Bayes preformed better in Precision metric, but worse in Recall metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier, lr_train = best_models[3]\n",
    "nb_classifiers, nb_train = best_models[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_target = split_data(lr_train, \"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier.fit(train_features, train_target)\n",
    "lr_classifier.evaluate(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifiers.fit(train_features, train_target)\n",
    "nb_classifiers.evaluate(train_features, train_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the models on test data we can see that they performed as expected. Logistic regression preformed better in all metrics, but precision where Naive Bayes performed better achieving a score of 0.97."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inteligent_systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "156b96cfb6bc5be6bb8885ba217766dffb33eba0527adf7df4ebbeb6f4231f5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
